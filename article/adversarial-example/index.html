<!DOCTYPE html><html><head><title>Adversarial examples</title><meta charset="UTF-8"><meta name="robots" content="index, follow"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="author" content="Steven Baldasty"><meta property="og:image" content><link rel="icon" type="image/x-icon" href="/logo.png"><link rel="stylesheet" href="/site.css"></head><body><div class="brand"><img src="/logo.png" title="Website logo" alt="Three stacked blue boxes with binary digits on them" width="86" height="86"><div class="exposition"><div class="title">Bit flipping laboratory &amp; Personal website</div><div class="caption">No sneaky cookies haunt these pages, but whether someone tracks you I do not know. Information flows through many channels, and every action leaves a trace.</div></div></div><div class="nav"><a href="/">Home</a><a href="/article/">Articles</a><a href="/media/">Media</a></div><hr><div class="article"><div class="date">2025-04-16</div><div class="summary">Adversarial examples derived from MNIST. Time series plotting model confidence deterioration over the iterative tuning of the examples. Heatmaps showing how choices of &epsilon; and &alpha; affect the resulting example's potency. Inspired by an Adversarial AI course at UVM.</div><h1>Adversarial examples</h1><p>Imagine an <a href="https://levity.ai/blog/how-do-image-classifiers-work" target="blank_">image classifier</a> built from a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target="blank_">convolutional neural network</a>. The classifier knows about a handful of <a href="https://en.wikipedia.org/wiki/Label_(machine_learning)" target="blank_">labels</a>, or things that an image could possibly be. It accepts an image as input, and outputs a probability for each label that the image is an example of that label. The resulting list of probabilities is called a <a href="https://en.wikipedia.org/wiki/Confidence_vector" target="blank_">confidence vector</a>, because it reflects the classifier's <i>confidence</i> that about what the image is. The label with the highest probability is understood to be the classifier's best guess.</p><p>An <i>adversarial example</i> is an image perturbed in a careful way to give the classifier high confidence that the image is something it is not. However from a human's perspective, the adversarial example looks almost exactly the same as the original image. It may appear slightly blurrier or have a small meaningless looking artifact, depending on the technique used to create it.</p><p>GitHub has no shortage of <a href="https://github.com/cleverhans-lab/cleverhans" target="blank_">repositories</a> about creating adversarial examples, and I take from some preexisting code as a starting point. I look at a technique that minimizes the classifier's confidence in the true label and offer some interesting data visualizations I have not seen elsewhere. I may later describe additional techniques that maximize the classifier's confidence in a particular chosen false label, or that spread the classifier's confidence evenly across all the labels.</p><p>To maintain focus, I only include parts of the source code that help explain and clarify the experiments. For complete source code, see the <a href="https://github.com/sbaldasty/adversarial-ai/blob/main/adversarial-example.ipynb" target="blank_">Jupyter notebook</a> on which this article is based.</p><h2>Sample images and model</h2><p>The <a href="https://www.kaggle.com/datasets/hojjatk/mnist-dataset" target="blank_">MNIST</a> dataset is a public dataset, formidable to classifiers in its day, but now commonly used for simple examples like mine. it contains many images of handwritten digits. The ubiquitous <a class="tech" href="https://pytorch.org/" target="blank_" title="Machine learning library for python">pytorch</a> library for <a class="tech" href="https://www.python.org/" target="blank_" title="Fairly ubiquitous programming language">python</a> even has built-in functions for loading it.</p><p>I randomly select six <code>images</code> and their corresponding <code>labels</code> from <a href="https://www.kaggle.com/datasets/hojjatk/mnist-dataset" target="blank_">MNIST</a>; however the selection is not <i>quite</i> random, because I choose the random seed deliberately so as to achieve a certain diversity in effects that I describe later.</p><pre class="snippet">mnist = MNIST(
    root=DATA_PATH,
    train=True,
    download=True,
    transform=ToTensor())

loader = DataLoader(
    dataset=mnist,
    batch_size=N_SAMPLES,
    shuffle=True)

images, labels = next(iter(loader))
labels = [label.item() for label in labels]</pre><p>Surprisingly and somewhat annoyingly for the purposes of a simple example, the extensive repository of pre-trained <a class="tech" href="https://pytorch.org/" target="blank_" title="Machine learning library for python">pytorch</a> models at <a href="https://pytorch.org/hub/" target="blank_">PyTorch Hub</a> does not include a <a href="https://www.kaggle.com/datasets/hojjatk/mnist-dataset" target="blank_">MNIST</a> classifier. I borrow a <code>model</code> from <a href="https://github.com/sarathknv/adversarial-examples-pytorch" target="blank_">Sarath Chandra Kothapalli</a> and reduce it to a single sequential module, just for clarity of presentation. The model is more complicated than necessary, but standard and effective.</p></p>Incidentally, Sarath explores adversarial examples on <a href="https://www.kaggle.com/datasets/hojjatk/mnist-dataset" target="blank_">MNIST</a> too. Their work is worth checking out.</p><pre class="snippet">model = Sequential(
    Conv2d(1, 32, kernel_size=3, padding=1),
    ReLU(),
    Conv2d(32, 32, kernel_size=3, padding=1),
    ReLU(),
    MaxPool2d(kernel_size=2),
    Conv2d(32, 64, kernel_size=3, padding=1),
    ReLU(),
    Conv2d(64, 64, kernel_size=3, padding=1),
    ReLU(),
    MaxPool2d(kernel_size=2),
    Flatten(),
    Linear(7 * 7 * 64, 200),
    ReLU(),
    Linear(200, 10),
    Softmax(dim=1))</pre><p>The experiments revolve around the classifier's confidence, so I define a function to calculate the confidence vector for an image. The function takes an image as input, and returns the classifier's confidence vector and prediction.</p><pre class="snippet">def calculate_confidence(image):
    unsqueezed_image = image.unsqueeze(0)
    output = model(unsqueezed_image)
    arrays = output.detach().numpy()
    return output, arrays[0]</pre><p>Just to concretize my progress and establish a baseline for comparison, I exercise the classifier on the six images I selected. I ask the classifier to label each image. I display the image, the true label, and the predicted label all together.</p><pre class="snippet">for i in range(N_SAMPLES):
    output, confidence = calculate_confidence(images[i])
    prediction = np.argmax(confidence)
    plt.figure(figsize=THUMBNAIL_SIZE)
    plt.imshow(images[i].squeeze(), cmap=&#39;gray&#39;)
    plt.axis(&#39;off&#39;)

    plt.title(
        f&#39;True:{labels[i]} Pred:{prediction}&#39;,
        fontsize=8)

    plt.savefig(
        f&#39;{OUT_PATH}/baseline_{i}.png&#39;,
        transparent=True,
        bbox_inches=&#39;tight&#39;,
        pad_inches=0.1)

    plt.show()</pre><div class="dense-gallery"><img src="baseline_0.png" title="Sample image #1" alt="True label 9, predicted label 9" width="97" height="113"><img src="baseline_1.png" title="Sample image #2" alt="True label 4, predicted label 4" width="97" height="113"><img src="baseline_2.png" title="Sample image #3" alt="True label 2, predicted label 2" width="97" height="113"><img src="baseline_3.png" title="Sample image #4" alt="True label 1, predicted label 1" width="97" height="113"><img src="baseline_4.png" title="Sample image #5" alt="True label 4, predicted label 6" width="97" height="113"><img src="baseline_5.png" title="Sample image #6" alt="True label 3, predicted label 3" width="97" height="113"></div><p>The classifier's predicted labels match the true labels in all but one case. <a href="https://www.kaggle.com/datasets/hojjatk/mnist-dataset" target="blank_">MNIST</a> does not contain many images where the classifier predicts incorrectly. I deliberately include such an image to show what happens differently in the visualizations that follow.</p><h2>Untargeted iterative FSGM</h2><p>Of the many techniques for creating adversarial examples, I choose the untargeted <i>iterative fast gradient sign method</i>. It iteratively perturbs the image against the direction of the gradient of the loss function with respect to the image.</p><p>A parameter called &alpha; proscribes the <i>distance</i> to move in that direction in a single iteration. Put another way, &alpha; controls how much should each pixel be perturbed in a single iteration. Another parameter called &epsilon; bounds the maximum distance a pixel may be perturbed over all the iterations combined. I have seen different approaches to choosing the number of iterations, but they usually depend on &epsilon; and &alpha;. For now I choose a simple ratio.</p><pre class="snippet">EPSILON = 0.07
ALPHA = .006

iterations = int(EPSILON / ALPHA)</pre><p>To implement FSGM, I borrow and adapt some code from an interactive demonstration by <a href="https://github.com/sarathknv/adversarial-examples-pytorch" target="blank_">Sarath Chandra Kothapalli</a>. For each of the <code>iterations</code>, the attack gets the confidence vector from the classifier. It uses the <code>loss</code> to calculate the <code>perturbation</code>, ensures the <code>perturbation</code> keeps the images within the bounds of <code>epsilon</code>, and then applies it to the <code>image</code>.</p><pre class="snippet">def attack(image, label, epsilon, alpha, iterations):
    criterion = CrossEntropyLoss()
    orig = image
    image = image.clone().detach().requires_grad_(True)
    label = torch.tensor([label])
    confidences = []
    for _ in range(iterations):
        output, confidence = calculate_confidence(image)
        loss = criterion(output, label)
        loss.backward()
        perturbation = alpha * torch.sign(image.grad.data)

        perturbation = torch.clamp(
            image.data + perturbation - orig,
            min=-epsilon,
            max=epsilon)

        image.data = orig + perturbation
        image.grad.data.zero_()
        confidences.append(confidence)

    image = image.detach().squeeze()
    return image, confidences, np.argmax(confidence)</pre><p>I demonstrate the attack on the six images, showing the perturbed images, their true labels, and the classifier's prediction all together. I tuned &epsilon; and &alpha; very particularly, to get a small but noticeable effect on the images. Notice the blotchiness in the background, as opposed to the pure black background in the unperturbed images. The classifier predicts incorrectly now, most of the time. I notice the classifier seems drawn toward 3 and 6, but I do not know if there is anything to be said about that.</p><pre class="snippet">perturbed = []
confidences = []
predictions = []

for i in range(N_SAMPLES):
    output, confidence, prediction = attack(
        images[i], labels[i], EPSILON, ALPHA, iterations)

    perturbed.append(output)
    confidences.append(confidence)
    predictions.append(prediction)

    plt.figure(figsize=THUMBNAIL_SIZE)
    plt.axis(&#39;off&#39;)
    plt.imshow(perturbed[i], cmap=&#39;gray&#39;)

    plt.title(
        f&#39;True:{labels[i]} Pred:{predictions[i]}&#39;,
        fontsize=8)

    plt.savefig(
        f&#39;{OUT_PATH}/untargeted_{i}.png&#39;,
        transparent=True,
        bbox_inches=&#39;tight&#39;,
        pad_inches=0.1)

    plt.show()</pre><div class="dense-gallery"><img src="untargeted_0.png" title="Modified image #1" alt="True label 9, predicted label 3" width="97" height="113"><img src="untargeted_1.png" title="Modified image #2" alt="True label 4, predicted label 6" width="97" height="113"><img src="untargeted_2.png" title="Modified image #3" alt="True label 2, predicted label 3" width="97" height="113"><img src="untargeted_3.png" title="Modified image #4" alt="True label 1, predicted label 3" width="97" height="113"><img src="untargeted_4.png" title="Modified image #5" alt="True label 4, predicted label 6" width="97" height="113"><img src="untargeted_5.png" title="Modified image #6" alt="True label 3, predicted label 3" width="97" height="113"></div><p>I may return to explore other techniques in the future. A <i>targeted</i> attack that makes the classifier predict a chosen label is possible; as are attacks that try to distribute the classifier's confidence evenly across all the labels.</p><h2>Visualizing the attack</h2><p>I devote the rest of the article to making some interesting visualizations I have not seen elsewhere. In the previous section, I captured the confidence vector at each iteration of the attack. Now, I plot the confidence vectors for each label over the iterations.</p><pre class="snippet">for i in range(N_SAMPLES):
    plt.figure(figsize=SERIES_SIZE)

    plt.title(
        f&#39;True: {labels[i]}   Pred:{predictions[i]}&#39;,
        fontsize=10)

    plt.xlabel(&#39;Iteration&#39;)
    plt.ylabel(&#39;Confidence&#39;)
    plt.xticks(list(range(iterations)))

    for j in range(10):
        cfs = [confidences[i][k][j] for k in range(iterations)]
        plt.plot(list(range(len(cfs))), cfs, label=f&#39;Label {j}&#39;)

    plt.legend(loc=&#39;upper left&#39;, bbox_to_anchor=(1, 1.15))

    plt.savefig(
        f&#39;{OUT_PATH}/series_{i}.png&#39;,
        bbox_inches=&#39;tight&#39;,
        pad_inches=0.1,
        transparent=True)
    
    plt.show()</pre><div class="dense-gallery"><img src="series_0.png" title="Confidence series for image #1" alt="Classifier changes prediction from 9 to 3" width="420" height="236"><img src="series_1.png" title="Confidence series for image #1" alt="Classifier changes prediction from 4 to 6" width="420" height="236"><img src="series_2.png" title="Confidence series for image #1" alt="Classifier changes prediction from 2 to 3" width="420" height="236"><img src="series_3.png" title="Confidence series for image #1" alt="Classifier changes prediction from 1 to 3" width="420" height="236"><img src="series_4.png" title="Confidence series for image #1" alt="Classifier retains its incorrect prediction of 6" width="420" height="236"><img src="series_5.png" title="Confidence series for image #1" alt="Classifier retains its correct prediction of 3" width="420" height="236"></div><p>In the first four examples, the classifier's confidence in the true label decreases over the iterations, and is eventually eclipsed by its confidence in some other label. I think it is interesting that the trend in confidence looks somewhat sigmoid-like, instead of linear. I also think it is interesting that the classifier consistently trends towards high confidence in a <i>single</i> incorrect label, instead of towards greater uncertainty.</p><p>In the fifth example, the classifier predicts an incorrect label from the beginning, and its confidence in that label only increases across iterations. The sixth example shows an attack that was not strong enough to dissuade the classifier from its correct prediction.</p><h2>Parameter choices</h2><p>The last experiment I try is a grid search over &epsilon; and &alpha; to see what happens to the classifier's confidence in the <i>true label</i> for different values of these parameters.</p><pre class="snippet">epsilons = np.arange(0.0001, 0.1501, 0.0025)
alphas = np.arange(0.0001, 0.1501, 0.0025)

output = np.full(
    (N_SAMPLES, len(epsilons), len(alphas)),
    np.nan)

grid = product(
    enumerate(zip(images, labels)),
    enumerate(epsilons),
    enumerate(alphas))

for (k, (image, label)), (i, e), (j, a) in grid:
    if a &gt; e: continue
    its = int(e / a) + 1
    _, cv, _ = attack(image, label, e, a, its)
    output[k][i][j] = cv[-1][label]</pre><p>Setting up the visuals is a little tricky. The idea is to have &epsilon; and &alpha; along the axes, and the <i>heat</i> represents the classifier's confidence in the true label for an adversarial example trained using the exact same technique as before, but with varied parameters.</p><pre class="snippet">INTERVAL = 10

def ticks(lst):
    return np.arange(0, len(lst), INTERVAL)

def tick_labels(lst):
    return [f&#39;{lst[x]:.2f}&#39; for x in range(0, len(lst), INTERVAL)]

for i in range(N_SAMPLES):
    plt.figure(figsize=HEATMAP_SIZE)
    plt.imshow(output[i], cmap=&#39;hot&#39;, interpolation=&#39;nearest&#39;)
    plt.colorbar().set_label(&#39;Confidence&#39;)
    plt.clim(0.0, 1.0)
    plt.xlabel(&#39;Alpha&#39;)
    plt.ylabel(&#39;Epsilon&#39;)

    plt.xticks(
        ticks=ticks(alphas),
        labels=tick_labels(alphas),
        rotation=90)

    plt.yticks(
        ticks=ticks(epsilons),
        labels=tick_labels(epsilons))

    inset_ax = inset_axes(
        parent_axes=plt.gca(),
        width=&#39;35%&#39;,
        height=&#39;35%&#39;,
        loc=&#39;upper right&#39;)

    inset_ax.imshow(images[i].squeeze(), cmap=&#39;gray&#39;)
    inset_ax.axis(&#39;off&#39;)

    plt.savefig(
        f&#39;{OUT_PATH}/heatmap_{i}.png&#39;,
        bbox_inches=&#39;tight&#39;,
        pad_inches=0.1,
        transparent=True)

    plt.show()</pre><div class="dense-gallery"><img src="heatmap_0.png" title="Heatmap for image #1" width="389" height="316"><img src="heatmap_1.png" title="Heatmap for image #2" width="389" height="316"><img src="heatmap_2.png" title="Heatmap for image #3" width="389" height="316"><img src="heatmap_3.png" title="Heatmap for image #4" width="389" height="316"><img src="heatmap_4.png" title="Heatmap for image #5" width="389" height="316"><img src="heatmap_5.png" title="Heatmap for image #6" width="389" height="316"></div><p>The pure white triangle in the upper right represents the region where &epsilon; &lt; &alpha;. No adversarial examples can be created under that restriction, because the total perturbation is bounded by an amount less than the perturbation applied at each iteration.</p><p>The fifth plot is different from the others, being almost all black. In this example, the classifier never predicted the correct label in the first place.</p><p>All the other plots look similar. Much of the space looks like a sequence of triangles with narrower and narrower bases. This surprised me at first, but it makes sense. It boils down to the number of iterations that are allowed, which in these experiments is &epsilon; / &alpha;. More iterations generally produces more effective adversarial examples. The <i>jumps</i> in classifier confidence are where the ratio permits a new number of iterations.</p><h2>Mere curiosities</h2><p>Some hacks succeed less frequently than in the past because effective defenses against them become common practice. In the 1990s when many web applications stored user-submitted form fields in databases, people invented <a href="https://en.wikipedia.org/wiki/SQL_injection" target="blank_">SQL injection</a>. They entered specially crafted text into form fields that tricked the application into running arbitrary queries and commands against the database. SQL injection depends on easy-to-make programming mistakes, and <i>most</i> development teams today use web frameworks or libraries that help them get it right.</p><p>Other hacks become outright obsolete because the technology they depend on changes in a fundamental way. In the 1960s and beyond, people invented <a href="https://en.wikipedia.org/wiki/Phreaking" target="blank_">phreaking</a>. They exploited the telephone system by playing specially crafted audible tones that that tricked the system into making free long distance calls. By the 1990s though, the telephone system had changed to superior digital signaling. Phreaking is no longer possible, but it remains an interesting historical curiosity.</p><p>Classifier training techniques that imperfectly resist adversarial examples already exist, as a <a href="https://arxiv.org/pdf/2006.16545" target="blank_">paper</a> from the Adversarial AI course discusses. Adversarial example resistant classifiers may not see widespread deployment though. Decision makers may be unaware of the risks. Resistant classifiers may take more time and resources to train. They may perform slightly worse on actual images than their vulnerable counterparts. Also, adversarial examples may not be of concern in the environment in which a classifier is deployed.</p><p>However, I believe that adversarial examples face a more existential problem. Consider that humans in 2025 can classify images far better than any state-of-the-art classifier. Furthermore, humans need far fewer reference examples, and can learn new labels ad hoc. Their superior image classification abilities serve as a witness that better image classification techniques are possible. Likely those more human-like solutions will still be representable as artificial neural networks, but ones with novel architectures, and training processes very different from back-propagation. They may summarily replace the solutions of today: and to the point, adversarial examples will not fool them because adversarial examples do not fool humans.</p></div><hr class="top-gapped"><div class="author"><div class="exposition"><div class="title">Steven Baldasty</div><div class="caption">Proud father, Barefoot runner, Chocolate enthusiast, Seasoned software engineer, Starry eyed PhD student, Novice human</div></div><img src="/selfie.png" title="Sketch courtesy of my daughter" alt="Handsome brown haired man with glasses" width="80" height="88"></div></body></html>